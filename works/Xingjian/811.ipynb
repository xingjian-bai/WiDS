{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"top\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Woman Life Freedom</b></div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align:center;\">\n","  <img src=\"https://www.cfg.polis.cam.ac.uk/sites/www.cfg.polis.cam.ac.uk/files/styles/leading/public/shutterstock_2214441509.png?itok=8kwjDfB1\" alt=\"woman_life_freedom\">\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align: justify;\">\n","This notebook is dedicated to the brave women of Iran who are fighting for their freedom. Despite facing significant obstacles, Iranian women have continued to stand up for their rights and demand greater freedom and equality. We recognize that the struggle for women's rights is ongoing and that there is much work to be done. By supporting the fight for women's life and freedom, we are hoping to create a better future for all, one in which every person has the opportunity to live a free and fulfilling life.</div>"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"top\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>WiDS 2023</b></div>"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"1.2\"></a>\n","<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #155D07; background-color: #ffffff;\"><b>WiDS 2023</b> Bayesian Optimization for CatBoost Hyperparameter Tuning and ...</h2>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align: justify;\">In this Kaggle notebook, we have employed a variety of advanced machine learning techniques to improve our model's performance. Firstly, we have used <b>Bayesian optimization</b> to tune the hyperparameters of our <b>CatBoost model</b>, which is a powerful gradient boosting algorithm. This approach enables us to automatically search the hyperparameter space, saving us significant amounts of time and manual effort. \n","Furthermore, we have utilized <b>Adversarial Validation</b>, a technique that involves training a model to differentiate between training and test data. This approach allows us to identify whether our model is overfitting or underfitting the training data, thus improving its overall robustness.\n","Additionally, we have implemented <b>Explainable Machine Learning using SHAP</b>, a method that provides insights into how different features affect our model's predictions. This enables us to understand and interpret our model's behavior more effectively, improving our ability to make informed decisions based on its outputs.\n","We have also employed <b>Pseudo Labeling</b>, a technique that involves using a model's predictions on unlabeled data to generate new labeled data. This approach can significantly increase the amount of training data available to us, improving our model's performance on the test data.\n","Lastly, we have used <b>Ensemble Learning</b>, which involves combining the outputs of multiple models to create a final prediction. This approach can improve the overall accuracy and robustness of our model, particularly in cases where individual models may struggle to capture the full complexity of the data.</div>"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"top\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Table of content</b></div>\n","\n","<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n","    \n","<a id=\"TOC\"></a>\n","## Table of Content\n","* [Importing Required Libraries](#lib)\n","* [Reading Dataset](#read_data)\n","* [Processing Dataset](#process)\n","* [Plitting the Dataset](#split)\n","* [Adversarial Validation](#adv)\n","* [Bayesian Optimization for CatBoost](#bocat)\n","* [Feature Importance](#fi)\n","* [Explainability](#xml)\n","* [Pseudo Labeling](#PL)\n","* [Ensemble Learning](#EL)\n","* [Submission](#submit)\n","* [List of Kaggle Notebooks Used as a Reference](#list)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"lib\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Importing Required Libraries</b></div> "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:44:08.232162Z","iopub.status.busy":"2023-02-14T19:44:08.231224Z","iopub.status.idle":"2023-02-14T19:44:09.736713Z","shell.execute_reply":"2023-02-14T19:44:09.735356Z","shell.execute_reply.started":"2023-02-14T19:44:08.232048Z"},"papermill":{"duration":0.024582,"end_time":"2023-01-06T18:22:28.218733","exception":false,"start_time":"2023-01-06T18:22:28.194151","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_columns', None)\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import LabelEncoder\n","from catboost import CatBoostRegressor"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"read_data\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Reading Dataset</b></div> "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:44:09.739529Z","iopub.status.busy":"2023-02-14T19:44:09.739013Z","iopub.status.idle":"2023-02-14T19:45:00.522613Z","shell.execute_reply":"2023-02-14T19:45:00.521367Z","shell.execute_reply.started":"2023-02-14T19:44:09.739484Z"},"papermill":{"duration":20.398385,"end_time":"2023-01-06T18:22:48.634146","exception":false,"start_time":"2023-01-06T18:22:28.235761","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_raw = pd.read_csv('/kaggle/input/widsdatathon2023/train_data.csv', parse_dates=[\"startdate\"])\n","test_raw = pd.read_csv('/kaggle/input/widsdatathon2023/test_data.csv', parse_dates=[\"startdate\"])\n","submit = pd.read_csv('/kaggle/input/widsdatathon2023/sample_solution.csv')\n","target = 'contest-tmp2m-14d__tmp2m'\n","\n","train_raw.head()"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"process\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Preprocessing Dataset</b></div> "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:45:00.524743Z","iopub.status.busy":"2023-02-14T19:45:00.524118Z","iopub.status.idle":"2023-02-14T19:45:00.541211Z","shell.execute_reply":"2023-02-14T19:45:00.539788Z","shell.execute_reply.started":"2023-02-14T19:45:00.524708Z"},"trusted":true},"outputs":[],"source":["def rmse(actual, predicted):\n","    return mean_squared_error(actual, predicted, squared=False)\n","\n","def location_nom(train, test):\n","    # Ref: https://www.kaggle.com/code/flaviafelicioni/wids-2023-different-locations-train-test-solved\n","    scale = 14\n","\n","    train.loc[:,'lat']=round(train.lat,scale)\n","    train.loc[:,'lon']=round(train.lon,scale)\n","    test.loc[:,'lat']=round(test.lat,scale)\n","    test.loc[:,'lon']=round(test.lon,scale)\n","\n","    all_df = pd.concat([train, test], axis=0)\n","    all_df['loc_group'] = all_df.groupby(['lat','lon']).ngroup()\n","    train = all_df.iloc[:len(train)]\n","    test = all_df.iloc[len(train):].drop(target, axis=1)\n","    \n","    return train, test\n","\n","def categorical_encode(train, test):\n","    le = LabelEncoder()\n","    train['climateregions__climateregion'] = le.fit_transform(train['climateregions__climateregion'])\n","    test['climateregions__climateregion'] = le.transform(test['climateregions__climateregion'])\n","    return train, test\n","    \n","def fill_na(df):\n","    df = df.sort_values(by=['loc_group', 'startdate']).ffill()\n","    return df\n","\n","def creat_new_featute(df):\n","    df['year'] = df['startdate'].dt.year\n","    df['month'] = df['startdate'].dt.month\n","    df['day_of_year'] = df['startdate'].dt.dayofyear\n","    # df['day_of_week'] = df['startdate'].dt.dayofweek\n","    # df['week_of_year'] = df['startdate'].dt.isocalendar().week\n","    return df\n","\n","def feature_engineering(train_raw, test_raw):\n","    train, test = location_nom(train_raw, test_raw)\n","    train = fill_na(train)\n","    train = creat_new_featute(train)\n","    test = creat_new_featute(test)\n","    train, test = categorical_encode(train, test)\n","\n","    drop_cols = ['index', 'startdate', 'lat', 'lon', target]\n","    features = [col for col in train.columns if col not in drop_cols]\n","    X = train[features]\n","    X_test = test[features]\n","    y = train[target]\n","\n","    return X, y, X_test"]},{"cell_type":"markdown","metadata":{},"source":["# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Train and Validation</b></div> "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:45:00.54627Z","iopub.status.busy":"2023-02-14T19:45:00.545867Z","iopub.status.idle":"2023-02-14T19:45:04.680531Z","shell.execute_reply":"2023-02-14T19:45:04.679313Z","shell.execute_reply.started":"2023-02-14T19:45:00.546238Z"},"trusted":true},"outputs":[],"source":["X, y, X_test = feature_engineering(train_raw.copy(), test_raw.copy())\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n","print(f'Train_shape: {X_train.shape}    |   Val_shape: {X_val.shape}    |   Test_shape: {X_test.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"adv\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Adversarial Validation</b></div>\n","\n","Adversarial Validation is a technique used to ensure that the distribution of data in the training set is similar to that of the test set. This is important because if the data in the training set is not representative of the test set, the model's predictions may not be accurate.\n","\n","To perform Adversarial Validation, the following steps are taken:\n","\n","1. Combine the train and test features into a single set\n","1. Create a target label to indicate whether a sample is from the train or test set\n","1. Build a model to classify samples as belonging to the train or test set\n","\n","If the model is able to accurately distinguish between train and test samples, this indicates that there are features in the data that are different between the two sets. Adversarial Validation can be used to identify these features by using the feature importance generated by the model and evaluating the separation between the train and test datasets using the AUC metric.\n","\n","Reference: Pan, J., Pham, V., Dorairaj, M., Chen, H., & Lee, J. Y. (2020). Adversarial validation approach to concept drift problem in user targeting automation systems at uber. [arXiv preprint arXiv:2004.03045](https://arxiv.org/abs/2004.03045)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:45:04.682356Z","iopub.status.busy":"2023-02-14T19:45:04.681976Z","iopub.status.idle":"2023-02-14T19:45:05.557422Z","shell.execute_reply":"2023-02-14T19:45:05.556071Z","shell.execute_reply.started":"2023-02-14T19:45:04.682318Z"},"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","\n","def run_adversial_validation(train_X_ml, test_X_ml):\n","    \n","    lgb_params = {'n_estimators':100,\n","                'boosting_type': 'gbdt',\n","                'objective': 'binary',\n","                'metric': 'auc',\n","                'verbose': 0\n","                    }\n","    # combine train & test features, create label to identify test vs train\n","    ad_y = np.array([1]*train_X_ml.shape[0] + [0]*test_X_ml.shape[0])\n","    ad_X = pd.concat([train_X_ml, test_X_ml])\n","\n","    # evaluate model performance using cross-validation\n","    lgb_data = lgb.Dataset(ad_X, ad_y)\n","    cv_lgb = lgb.cv(lgb_params, lgb_data)\n","\n","    print(\"Adversarial Validation AUC Score: {}\".format(cv_lgb['auc-mean'][-1]))\n","    \n","    # train model & get feature importance\n","    ad_val_mod = lgb.train(lgb_params, lgb_data)\n","    \n","    print(pd.DataFrame(\n","        {'feat':ad_X.columns, \n","         'imp':ad_val_mod.feature_importance()}).sort_values('imp', ascending = False))\n","    \n","    return ad_val_mod"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-02-14T19:45:05.559904Z","iopub.status.busy":"2023-02-14T19:45:05.559383Z","iopub.status.idle":"2023-02-14T19:46:20.049261Z","shell.execute_reply":"2023-02-14T19:46:20.048235Z","shell.execute_reply.started":"2023-02-14T19:45:05.559854Z"},"trusted":true},"outputs":[],"source":["ad_val_mod = run_adversial_validation(X_train, X_test)"]},{"cell_type":"markdown","metadata":{},"source":["It seems that there is a concept drift between train and test dataset and removing features like \"contest-pevpr-sfc-gauss-14d__pevpr\" and \"nmme0-tmp2m-34w__cancm30\" might help to increase the performance. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:46:20.055891Z","iopub.status.busy":"2023-02-14T19:46:20.053227Z","iopub.status.idle":"2023-02-14T19:46:20.06217Z","shell.execute_reply":"2023-02-14T19:46:20.060857Z","shell.execute_reply.started":"2023-02-14T19:46:20.055849Z"},"trusted":true},"outputs":[],"source":["# X.drop(['contest-pevpr-sfc-gauss-14d__pevpr','nmme0-tmp2m-34w__cancm30'], inplace = True)\n","# X_test.drop(['contest-pevpr-sfc-gauss-14d__pevpr','nmme0-tmp2m-34w__cancm30'], inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"bocat\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Bayesian Optimization for CatBoost</b></div> \n","\n","[Here's](https://en.wikipedia.org/wiki/Bayesian_optimization) a wikipedia article about Bayesian Optimization, it's essentially a way to find good parameters by searching for these parameters sequentially. So the next parameter search values depend on the performance of the previous parameter values. This is a popular technique for finding optimal parameters. This may take some time to run. Also, you can tune many parameters, below are just some of the parameters that I choose to tune. For this notebook, I'm going to fix the number of estimators to be 100 to save time when running this notebook, but you can increase it to your liking or tune that parameter too if you'd like. You can change the number of iterations and initial points."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:46:20.067582Z","iopub.status.busy":"2023-02-14T19:46:20.066239Z","iopub.status.idle":"2023-02-14T19:46:20.376191Z","shell.execute_reply":"2023-02-14T19:46:20.375218Z","shell.execute_reply.started":"2023-02-14T19:46:20.067542Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# source: https://medium.com/ai-in-plain-english/catboost-cross-validated-bayesian-hyperparameter-tuning-91f1804b71dd\n","\n","X1, Y1 = X.copy(), y.copy()\n","\n","from catboost import Pool, cv, CatBoostRegressor\n","from bayes_opt import BayesianOptimization\n","from bayes_opt import BayesianOptimization as BO\n","import warnings\n","from sklearn.model_selection import * \n","from sklearn.metrics import *\n","\n","Use_BO = False\n","\n","if Use_BO:\n","    #n_estimators,\n","    # num_leaves\n","    def CB_opt(depth, learning_rate, subsample, l2_leaf_reg, model_size_reg): \n","\n","        scores = []\n","    #     skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1944)\n","        trainx, valx, trainy, valy = train_test_split(X1, Y1, test_size=0.33, random_state=42)\n","\n","        reg = CatBoostRegressor(   \n","                                        verbose = 0,\n","                                        #iterations=10,\n","                                        #n_estimators = 10,\n","                                        learning_rate = learning_rate,\n","                                        subsample = subsample, \n","                                        l2_leaf_reg = l2_leaf_reg,\n","                                        max_depth = int(depth),\n","                                        #num_leaves = int(num_leaves),\n","                                        random_state = 1212,\n","                                        #grow_policy = \"Lossguide\",\n","    #                                     max_bin = int(max_bin),  \n","                                        use_best_model = True, \n","                                        # bootstrap_type='Bayesian',\n","                                        loss_function='RMSE',\n","                                        model_size_reg = model_size_reg\n","                                    )\n","\n","        reg.fit(trainx, trainy, eval_set = (valx, valy))\n","        y_pred = reg.predict(valx)\n","        scores.append(rmse(valy, y_pred))\n","\n","        return 1/np.mean(scores)\n","\n","    #\"n_estimators\": (150,1200),\n","    # \"num_leaves\": (100,150),\n","    # \"max_bin\":(150,300),\n","    pbounds = {\n","               \"depth\": (6, 7),\n","               \"learning_rate\": (0.09, 0.0980689972639084),\n","               \"subsample\":(0.7, 0.800000011920929),\n","               \"l2_leaf_reg\":(2,4),\n","               \"model_size_reg\": (0.48, 0.5)\n","    }\n","\n","    optimizer = BayesianOptimization(f = CB_opt, pbounds = pbounds,  verbose = 2, random_state = 1212)\n","\n","    optimizer.maximize(init_points = 7, n_iter = 30, acq = 'ucb', alpha = 1e-6)\n","\n","    print(optimizer.max)\n","\n","    max_bo_params = optimizer.max['params']\n","\n","    max_bo_params"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:46:20.378356Z","iopub.status.busy":"2023-02-14T19:46:20.377733Z","iopub.status.idle":"2023-02-14T19:46:20.38532Z","shell.execute_reply":"2023-02-14T19:46:20.384477Z","shell.execute_reply.started":"2023-02-14T19:46:20.378297Z"},"trusted":true},"outputs":[],"source":["Use_BO_result = False\n","\n","if Use_BO_result:\n","    opt_params = {\n","              'iterations':2000,\n","              'verbose':0,\n","              'learning_rate' : max_bo_params['learning_rate'],\n","              'subsample' : max_bo_params['subsample'], \n","              'l2_leaf_reg' : max_bo_params['l2_leaf_reg'],\n","              'max_depth' : int(max_bo_params['depth']), \n","              'use_best_model' : True, \n","              'loss_function' : 'RMSE',\n","              'model_size_reg' : max_bo_params['model_size_reg']\n","             }\n","else:\n","    opt_params = {\n","          'iterations':25000,\n","          'verbose':0,\n","          'learning_rate' : 0.0980689972639084,\n","          'subsample' : 0.7443133148363695, \n","          'l2_leaf_reg' : 2.3722386345448316,\n","          'max_depth' : int(6.599144674342465),\n","          'use_best_model' : True, \n","          'loss_function' : 'RMSE',\n","          'model_size_reg' : 0.4833187897595954\n","         }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:46:20.389143Z","iopub.status.busy":"2023-02-14T19:46:20.387956Z","iopub.status.idle":"2023-02-14T19:57:52.816672Z","shell.execute_reply":"2023-02-14T19:57:52.815714Z","shell.execute_reply.started":"2023-02-14T19:46:20.389106Z"},"trusted":true},"outputs":[],"source":["## catBoost Pool object\n","train_pool = Pool(data=X1,label = Y1)\n","\n","X_train, X_test2, y_train, y_test = train_test_split(X1, Y1, test_size=0.33, random_state=42)\n","\n","bst = CatBoostRegressor(**opt_params)\n","bst.fit(train_pool, eval_set=(X_test2, y_test), plot=False,silent=True)\n","print(bst.get_best_score())"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"fi\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Feature Importance</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:57:52.818254Z","iopub.status.busy":"2023-02-14T19:57:52.817744Z","iopub.status.idle":"2023-02-14T19:57:53.871766Z","shell.execute_reply":"2023-02-14T19:57:53.870558Z","shell.execute_reply.started":"2023-02-14T19:57:52.818224Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","feature_importance = bst.feature_importances_\n","max_features = 50\n","sorted_idx = np.argsort(feature_importance)[-max_features:]\n","fig = plt.figure(figsize=(8, 12))\n","plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n","plt.yticks(range(len(sorted_idx)), np.array(X_val.columns)[sorted_idx])\n","plt.title('Feature Importance')"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"xml\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Machine Learning Explainability</b></div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align: justify;\">In this section, we showcase a sample of SHAP explainability evaluation for our model. SHAP (SHapley Additive exPlanations) is a popular approach for providing model interpretability by measuring the contribution of each feature to a prediction. This enables us to identify which features have the greatest impact on our model's outputs, and how they are related to the predicted values. \n","To perform the SHAP evaluation, we first generate a set of test data and extract the features we want to evaluate. We then use the SHAP library to compute the SHAP values for each feature, which represent the change in the predicted value as a result of changing the feature value while holding all other features constant. These values are visualized using a SHAP summary plot, which shows the features ranked by their importance and the direction of their impact on the predicted value. Through the SHAP summary plot, we can observe the top contributing features to our model's predictions, and how they are positively or negatively correlated with the output. This enables us to gain insights into the underlying relationships between the features and the target variable, improving our understanding of how our model makes its predictions. Overall, the SHAP explainability evaluation provides a valuable tool for gaining insights into the inner workings of our model and making informed decisions based on its outputs.</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:57:53.874145Z","iopub.status.busy":"2023-02-14T19:57:53.873199Z","iopub.status.idle":"2023-02-14T19:59:34.265359Z","shell.execute_reply":"2023-02-14T19:59:34.263397Z","shell.execute_reply.started":"2023-02-14T19:57:53.874108Z"},"trusted":true},"outputs":[],"source":["import shap\n","\n","explainer = shap.Explainer(bst)\n","shap_values = explainer(X1)\n","\n","# visualize the first prediction's explanation\n","shap.plots.waterfall(shap_values[0])"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"PL\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Pseudo Labeling and Postprocessing</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:59:34.269705Z","iopub.status.busy":"2023-02-14T19:59:34.268251Z","iopub.status.idle":"2023-02-14T19:59:35.954738Z","shell.execute_reply":"2023-02-14T19:59:35.953613Z","shell.execute_reply.started":"2023-02-14T19:59:34.269627Z"},"trusted":true},"outputs":[],"source":["# Pseudo Labelling\n","train_pseudo = X_test.copy()\n","ddf = pd.read_csv('/kaggle/input/wids-2023-sub3/submission (17).csv')\n","y_test_pred  = ddf[target] #bst.predict(X_test)\n","train_pseudo[target] = y_test_pred\n","train_mod = pd.concat([X_train.copy(), train_pseudo], axis=0).reset_index(drop=True)\n","features = [c for c in X_test.columns if (c != 'id')]\n","display(train_mod)\n","\n","XX = train_mod[features]\n","yy = train_mod[target]\n","y_oof_pred = np.zeros(len(yy))\n","\n","X_testt = X_test[features].values\n","y_test_pred2 = np.zeros(len(X_testt))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T19:59:35.956484Z","iopub.status.busy":"2023-02-14T19:59:35.956137Z","iopub.status.idle":"2023-02-14T20:08:37.430184Z","shell.execute_reply":"2023-02-14T20:08:37.428829Z","shell.execute_reply.started":"2023-02-14T19:59:35.956456Z"},"trusted":true},"outputs":[],"source":["yy[np.isnan(yy)] = 0\n","train_pool = Pool(data=XX,label = yy)\n","\n","X_train3, X_test3, y_trai3, y_test3 = train_test_split(XX, yy, test_size=0.33, random_state=42)\n","\n","bst2 = CatBoostRegressor(**opt_params)\n","bst2.fit(train_pool, eval_set=(X_test3, y_test3), plot=True,silent=True)\n","print(bst2.get_best_score())"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"EL\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Ensemble Learning</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-14T20:17:32.919927Z","iopub.status.busy":"2023-02-14T20:17:32.918867Z"},"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","\n","# set up parameters for LightGBM\n","params = {'boosting_type': 'gbdt',\n","          'objective': 'regression',\n","          'metric': 'rmse',\n","          'max_depth': 4,\n","          'num_leaves': 31,\n","          'learning_rate': 0.05,\n","          'feature_fraction': 0.9,\n","          'bagging_fraction': 0.8,\n","          'bagging_freq': 5,\n","          'early_stopping_round': 50,\n","          'n_estimators': 15000}\n","\n","reg_lgb = lgb.LGBMRegressor(**params)\n","\n","reg_lgb.fit(X_train3, y_trai3, eval_set=(X_test3, y_test3),verbose=100)\n","\n","y_pred_cat = bst2.predict(X_test)\n","\n","y_pred_lgb = reg_lgb.predict(X_test)\n","\n","ensemble_preds = y_pred_lgb*0.60+y_pred_cat*0.40"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submit_cat = submit.copy()\n","submit_cat[target] = y_pred_cat\n","submit_cat.to_csv('y_pred_cat.csv', index = False)\n","\n","submit_lgb = submit.copy()\n","submit_lgb[target] = y_pred_lgb\n","submit_lgb.to_csv('y_pred_lgb.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"submit\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Submission</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submit[target] = ensemble_preds\n","submit.to_csv('submission.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"list\"></a>\n","<div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>List of Kaggle Notebooks Used as a Reference</b></div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n","\n","* [[WiDS 2023] Simple basline - RMSE 1.14](https://www.kaggle.com/code/ducanger/wids-2023-simple-basline-rmse-1-14) by [DAT DO](https://www.kaggle.com/ducanger) used as base especially for preprocessing.\n","* [üî• EDA & ML on Game Play üéÆ (ongoing)](https://www.kaggle.com/code/nguyenthicamlai/eda-ml-on-game-play-ongoing) by [Nguyen Thi Cam Lai](https://www.kaggle.com/nguyenthicamlai) used for HTML-based headers\n","* [[WiDS 2021] Tips & Tricks (CatBoost Version)](https://www.kaggle.com/code/kooaslansefat/tips-tricks-catboost-version) used for hyperparameter tunning for CatBoost and adversarial validation\n","* [WiDS2023_Data_Buddies](https://www.kaggle.com/code/nicholasdominic/wids2023-data-buddies) by [Nicholas Dominic](https://www.kaggle.com/nicholasdominic) for Ensemble Learning"]},{"cell_type":"markdown","metadata":{},"source":["<center> <a href=\"#TOC\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to Table of Contents ‚¨ÜÔ∏è</a>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"border-radius:10px;border:#034914 solid;padding: 15px;background-color:aliceblue;font-size:90%;text-align:left\">\n","\n","<h4><b>Authors :</b> Mojgan Hashemian and Koorosh Aslansefat </h4>  \n","    \n","<center> <strong> If you liked this Notebook, please do upvote. </strong>\n","    \n","<center> <strong> If you have any questions, feel free to contact us! </strong>"]},{"cell_type":"markdown","metadata":{},"source":["<center> <img src=\"https://gregcfuzion.files.wordpress.com/2022/01/kind-regards-2.png\" style='width: 600px; height: 300px;'>"]}],"metadata":{"kernelspec":{"display_name":"wids","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"a4442760baeee514ff3d8d14cfc9d6d325f4d751deb91a18ba8c52416c0c6935"}}},"nbformat":4,"nbformat_minor":4}
