{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from catboost import CatBoostRegressor\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../../data/train_data.csv', parse_dates=[\"startdate\"])\n",
    "test_raw = pd.read_csv('../../data/test_data.csv', parse_dates=[\"startdate\"])\n",
    "submit = pd.read_csv('../../data/sample_solution.csv')\n",
    "target = 'contest-tmp2m-14d__tmp2m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n",
    "    return mean_squared_error(actual, predicted, squared=False)\n",
    "\n",
    "def location_nom(train, test):\n",
    "    # Ref: https://www.kaggle.com/code/flaviafelicioni/wids-2023-different-locations-train-test-solved\n",
    "    scale = 14\n",
    "\n",
    "    train.loc[:,'lat']=round(train.lat,scale)\n",
    "    train.loc[:,'lon']=round(train.lon,scale)\n",
    "    test.loc[:,'lat']=round(test.lat,scale)\n",
    "    test.loc[:,'lon']=round(test.lon,scale)\n",
    "\n",
    "    all_df = pd.concat([train, test], axis=0)\n",
    "    all_df['loc_group'] = all_df.groupby(['lat','lon']).ngroup()\n",
    "    train = all_df.iloc[:len(train)]\n",
    "    test = all_df.iloc[len(train):].drop(target, axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def categorical_encode(train, test, feature_name):\n",
    "    # le = LabelEncoder()\n",
    "    # train['climateregions__climateregion'] = ohe.fit_transform(train['climateregions__climateregion'])\n",
    "    # test['climateregions__climateregion'] = ohe.transform(test['climateregions__climateregion'])\n",
    "    # return train, test\n",
    "    # using OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "    train_encoded = ohe.fit_transform(train[[feature_name]])\n",
    "    test_encoded = ohe.transform(test[[feature_name]])\n",
    "    \n",
    "    train = train.drop([feature_name], axis=1)\n",
    "    test = test.drop([feature_name], axis=1)\n",
    "    \n",
    "    train_encoded = pd.DataFrame(train_encoded.toarray(), columns=ohe.get_feature_names_out([feature_name]))\n",
    "    test_encoded = pd.DataFrame(test_encoded.toarray(), columns=ohe.get_feature_names_out([feature_name]))\n",
    "    \n",
    "    train = pd.concat([train, train_encoded], axis=1)\n",
    "    test = pd.concat([test, test_encoded], axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "    \n",
    "def fill_na(df):\n",
    "    # TODO: fill na with mean or median\n",
    "    df = df.sort_values(by=['loc_group', 'startdate']).ffill()\n",
    "    return df\n",
    "\n",
    "def add_season(df):\n",
    "    month_to_season = {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 1,\n",
    "        4: 1,\n",
    "        5: 1,\n",
    "        6: 2,\n",
    "        7: 2,\n",
    "        8: 2,\n",
    "        9: 3,\n",
    "        10: 3,\n",
    "        11: 3,\n",
    "        12: 0,\n",
    "    }\n",
    "    df[\"season\"] = df[\"month\"].apply(lambda x: month_to_season[x])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "def encode_cyclical(df):\n",
    "    # encode the day with a period of 365\n",
    "    df[\"day_of_year_sin\"] = sin_transformer(365).fit_transform(df[\"day_of_year\"])\n",
    "    df[\"day_of_year_cos\"] = cos_transformer(365).fit_transform(df[\"day_of_year\"])\n",
    "\n",
    "    # encode the month with a period of 12\n",
    "    df[\"month_sin\"] = sin_transformer(12).fit_transform(df[\"month\"])\n",
    "    df[\"month_cos\"] = cos_transformer(12).fit_transform(df[\"month\"])\n",
    "\n",
    "def creat_new_featute(df):\n",
    "    df['year'] = df['startdate'].dt.year\n",
    "    df['month'] = df['startdate'].dt.month\n",
    "    df['day_of_year'] = df['startdate'].dt.dayofyear\n",
    "    add_season(df)\n",
    "    encode_cyclical(df)\n",
    "    return df\n",
    "\n",
    "#TODO: drop features with high correlation\n",
    "def feature_engineering(train_raw, test_raw):\n",
    "    train, test = location_nom(train_raw, test_raw)\n",
    "    train = fill_na(train)\n",
    "    train = creat_new_featute(train)\n",
    "    test = creat_new_featute(test)\n",
    "    train, test = categorical_encode(train, test, 'climateregions__climateregion')\n",
    "    train, test = categorical_encode(train, test, 'season')\n",
    "\n",
    "    # Xingjian: not drap lat and lon\n",
    "    drop_cols = ['index', 'startdate', target]\n",
    "    # drop_cols = ['index', 'startdate', 'lat', 'lon', target]\n",
    "    features = [col for col in train.columns if col not in drop_cols]\n",
    "    X = train[features]\n",
    "    X_test = test[features]\n",
    "    y = train[target]\n",
    "\n",
    "    return X, y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4442760baeee514ff3d8d14cfc9d6d325f4d751deb91a18ba8c52416c0c6935"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
